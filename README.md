# Lab-1-Neural-Network-Laksh


## Description
This project implements a **Feedforward Neural Network from scratch** using Python and NumPy, without any deep learning libraries. The network is trained using **forward pass, backpropagation, and gradient descent**.

The implementation demonstrates how neural networks learn by updating weights to minimize prediction error.

---

## Objective
To understand the working of neural networks by manually implementing:
- Forward propagation  
- Loss calculation  
- Backpropagation  
- Gradient descent weight updates  

---

## Algorithm Used
**Feedforward Neural Network with Backpropagation**

The network contains:
- Input layer (2 neurons)  
- Hidden layer (5 neurons)  
- Output layer (5 neurons)  

Sigmoid activation function is used. Mean Squared Error (MSE) is used as the loss function.

---

## Dataset
[0,0]
[0,1]
[1,0]
[1,1]


The network learns multiple logical patterns simultaneously using 5 output neurons.

---

## Features of Implementation

- Neural network built completely from scratch  
- No in-built deep learning libraries used  
- Weight updates printed after every epoch  
- Loss vs Epoch graph visualization  
- User input testing after training  

---

## How to Run

1. Open the Jupyter Notebook:
   `NN_From_Scratch_Lab1.ipynb`

2. Run all cells in order.

3. Observe:
   - Weight updates in each epoch  
   - Loss decreasing  
   - Final predictions  

4. Enter custom input values when prompted to test the trained network.

---

## Applications of Neural Networks

- Image recognition  
- Speech recognition  
- Fraud detection  
- Medical diagnosis  
- Recommendation systems  

---

## Conclusion
The neural network successfully learned multiple logical patterns using forward pass and backpropagation. Weight updates reduced the loss over epochs, demonstrating effective learning.
The dataset consists of all binary combinations of two inputs:


